---
title: "Predicting Instances of Death Caused by Heart Failure"
subtitle: "Using Machine Learning to predict death caused by heart failure using patient health data"
author: "Kyle Wu"
date: "UCSB Winter 2023"
bibliography: references.bib
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<center>
![](heartfail.jpg)
</center>


## Introduction

The goal of this project is to create a machine learning model that can successfully predict whether a patient will die due to heart failure based off of some patient history and vital signs.

### What is Heart Failure

Although heart failure sounds like the heart may have stopped, this is not the case. Heart failure, which is also known as congestive heart failure is a serious, incurable condition where the heart does not work properly and fails to pump blood sufficiently throughout the body for its needs. Heart failure may occur of the heart can't fill up with enough blood or if the heart is simply too weak to properly pump.

According to the Center for Disease Control and Prevention, more than 6 million adults in the United States suffer from heart failure.

According to the National Heart, Lung, and Blood Institute (NHLBI), "Heart failure may not cause symptoms right away. But eventually, you may feel tired and short of breath and notice fluid buildup in your lower body, around your stomach, or your neck." Heart failure can also eventually cause damage to other organs such as the liver or kidneys and lead to other conditions such as pulmonary hypertension, heart valve disease, and sudden cardiac arrest.

Although heart disease is incurable, the Mayo Clinic states that "Proper treatment can improve the signs and symptoms of heart failure and may help some people live longer," and that "Lifestyle changes - such as losing weight, exercising, and managing stress - can improve your quality of life" [@mayo]

### Why Predict Death by Heart Failure

Although heart failure may be incurable, it could still be beneficial for medical professionals to predict whether a patient may develop and potentially die from heart failure. For example, if a doctor can determine with high probability that a patient may develop heart failure later in life, they may be able to inform the patient so that they can make lifestyle changes early enough to prevent the most significant symptoms.

Additionally, although the body initially tries to mask the problem of heart failure through various mechanisms such as enlarging the heart, developing more muscle mass, or pumping faster, these solutions are all temporary and in these cases, heart failure will simply progress until the onset of more serious symptoms such as fatigue or breathing problems. Since treatment can often slow down the progression of heart failure, having a machine learning model that could successfully predict a person's chances of suffering and hence dying from heart failure would mean that we could increase early detection and likely catch more cases early on and slow the progression of the disease.

Since the data set I will use includes deaths as a result of heart failure, creating an effective machine learning model out of this data set would also allow doctors to preemptively begin treatment that may prevent the patient from dying due to heart failure.

### About the Data set

This data set was assembled as part of a study conducted on heart failure patients who were admitted to Institute of Cardiology and Allied hospital Faisalabad-Pakistan between April-December 2015 [@survive2017]. All patients in this case had left-ventricular systolic dysfunction, meaning that the left ventricle was unable to contract vigorously, which would indicate a pumping problem [@mayo]. Furthermore, patients in this study all fell into the New York Heart Association (NYHA) Functional Classification levels III and IV.

### Project Outline
This study will take on a number of steps in order to create machine learning models that will be allow us to predict the chance that a patient will die from heart failure. Since the data is formatted so that it is ready for analysis we will first explore the data through the use of graphs, which will allow us to see how heart failure deaths may relate to the different variables. We will then split our data into training and testing sets, create our recipe and use 10-fold cross validation. We will then implement 7 machine learning models: logistic regression, linear discriminant analysis, quadratic discriminant analysis, K-nearest neighbors, elastic net, random forests, and gradient-boosted decision trees. We will then evaluate the efficacy of the models before choosing our 2 best models to further analyze.


## Exploratory Data Analysis

### Loading in Packages and Data

We will first begin by loading in the packages we will use for the project and by loading raw heart failure data to the variable `heartfailure_data`.

```{r, message = FALSE}
# Loading in libraries we will be using 
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(knitr)
library(corrplot)
library(ggthemes)
library(gt)
library(gtExtras)
library(visdat)
library(fastDummies)
library(MASS)
library(glmnet)
library(ranger)
library(xgboost)
library(discrim)
library(doParallel)
library(themis)
library(kableExtra)
library(vip)
tidymodels_prefer()
```

```{r, message = FALSE}
# Read raw data into a data frame. 
heartfailure_data <- read_csv("heart_failure_clinical_records_dataset.csv")

head(heartfailure_data) %>%
  gt() %>%
  gt_theme_nytimes() %>%
  tab_header("Heart Failure Data") 
```

The data was obtained from the Kaggle Data set ["Heart Failure Prediction"](https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data), with the original data being from a [study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181001) conducted by Tanvir Ahmad, Assia Munir, Sajjad Haider Bhatti, Muhammad Aftab, and Muhammad Ali Raza. $$\\$$


### Tidying Our Data

We can now look at some basic information about the size of our data set:

```{r}
dim(heartfailure_data)
```

We can see that our data set has 299 observations to go along with 13 variables. Let us now take a look at a summary of our variables:

```{r, message = FALSE}
vis_dat(heartfailure_data)
```

We can see that our data does not include any missing data, so that is not something that we need to worry about. We also see that all of our data is of type numeric, even though some of our variables, including `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking`, and `DEATH_EVENT` are binary, so we will have to deal with that.

```{r}
# Converting categorical variables to factors.
heartfailure_data <- heartfailure_data %>%
  mutate(DEATH_EVENT = factor(DEATH_EVENT, levels = c("1", "0")),
         anaemia = factor(anaemia),
         diabetes = factor(diabetes),
         high_blood_pressure = factor(high_blood_pressure),
         sex = factor(sex),
         smoking = factor(smoking))
```

When looking at information about the original data set, I also noticed that `time` indicated either the number of days until the patients died, or the number of days until the patient was censored, which in this case simply means that they did not die. Due to this, I have decided that this information would not only be hard to interpret, it would also be irrelevant to whether the patient actually died of heart failure or not so will elect to remove that from the data set I will use for the machine learning models.

```{r}
heartfailure_data <- heartfailure_data %>% select(-time)
```

### Variable Breakdown

Now we are left with the following variables which will be utilized for the machine learning model: - `age`: The age of the patients in the study

-   `anaemia`: Patients were considered anemic (indicated by a 1) if their haematocrit levels were lower than 36%. (indicated by 0 if patient was not anemic).

-   `creatinine_phosphokinase`: The amount of creatinine phosphokinase (CPK) in the blood. CPK is often released into the blood when muscle tissue gets damaged.

-   `diabetes`: 1 if the patient has diabetes, 0 if patient does not have diabetes.

-   `ejection_fraction`: Indicates the percentage of blood the left ventricle pumped out upon each contraction.

-   `platelets`: Result of platelet count, which measure the number of platelets in the blood.

-   `serum_creatinine`: Creatinine levels in the blood. High serum creatinine levels indicate that the kidneys may not be functioning properly [@creatinine2019, @cpk2010].

-   `serum_sodium`: Results of a blood sodium test. Low serum sodium levels may be an indicator of heart failure [@sodium2018]

-   `sex`: 1 if the patient is male, 0 if the patient is female.

-   `smoking`: 1 if the patient smokes, 0 if the patient does not smoke.

-   `DEATH_EVENT`: 1 if the patient died during the course of the study, 0 if the patient did not die during the course of the study.

## Visual EDA

### Heart Failure Deaths Distribution

We will first look at the distribution of heart failure deaths

```{r}
ggplot(heartfailure_data, aes(x = as.factor(DEATH_EVENT), fill = "pink")) + 
  geom_bar() + 
  scale_fill_manual(values = "pink") +
  labs(title = "Distribution of Heart Failure Deaths", x = "Death Event", y = "Count") +
  theme(legend.position = "none")
```

From the histogram, we see that most of the patients in the study did not die during the duration of the study. In fact, of the 299 observations, 96 (32.1%) of the patients died and 203 (67.9%) of the patients did not die.

### Variable Correlation Plot

To see if there is any correlation between our numeric variables, we will now make a correlation heat map of the correlation between the predictors.

```{r}
heartfailcorr <- heartfailure_data %>% 
  select(where(is.numeric)) %>%
  cor() %>%
  corrplot(order = "AOE", addCoef.col = "black")
```

From the heat map, we see that there is little to no correlation between all of the numeric variables. At first it may seem suspicious that none of the variables are really correlated with each other but upon further understanding of all the numeric variables, the result makes sense. In this case, with the exception of age, the values we received were all from tests that the doctors would have performed. Since each test was used to measure completely different aspects of the patient's health, there is no reason to believe that any of the tests performed should have results that strongly correlate with one another.

### Age

It is often the case that older patients are more likely to die from medical conditions that may arise. From the box plot shown, we see that with heart failure, this is indeed the case. We see that the age of patients who died over the course of the study tended to lean quite a bit higher than for patients who did not die. Both the median age for the patients who died and the 75th percentile for patients who did not die was 65. The median age for patients who did not die was 60 years old.

```{r}
ggplot(data = heartfailure_data, aes(x = age, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() +
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Age Distribution of Patients who Lived/Died during Study", x = "Age", fill = "Death Event") 
```

### Sex

This data set was comprised of 105 women and 194 men. From the data, we also find that 31.96% of men and 32.38% of women in the data set died during the duration of the study.


```{r}
# make bar plot with sex distribution and likelihood of death by sex involved

sex_renamed <- heartfailure_data
levels(sex_renamed$sex)[levels(sex_renamed$sex) == "0"] <- "Female"
levels(sex_renamed$sex)[levels(sex_renamed$sex) == "1"] <- "Male"

ggplot(data = sex_renamed, aes(x = sex, group = DEATH_EVENT, fill = DEATH_EVENT)) + 
  geom_bar() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Distribution of Sex", y = "Count", fill = "Death Event")
```

### Creatinine Phosphokinase Levels

Creatinine Phosphokinase (CPK) is often released into the blood when muscle tissue gets damaged, making it a relatively good metric upon which heart failure can be diagnosed [@creatinine2019]. Depending on factors such as age, gender, and activity level, generally, 24-204 U/L is considered normal [@cpknorm]. Since all patients in the study were already experiencing heart failure and fell into NYHA classification levels III and IV, we would expect their CPK levels to be higher than the average population. We find that the average CPK levels for patients who died was 670 while average CPK levels for those who did not die was 540.

```{r}
ggplot(data = heartfailure_data, aes(x = creatinine_phosphokinase, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Creatinine Phosphokinase Distribution of Patients", x = "Creatinine Phosphokinase Levels (IU/L)", fill = "Death Event") 
```

### Ejection Fraction

Ejection fraction is a measure of the percentage of blood that the ventricles pump out upon each contraction, and a lower fraction would mean that an individual's heart was having difficulty keeping up with the body's needs and would be a strong indication of heart failure. A healthy male would have an ejection fraction in the range of 52%-72% and a healthy woman would have an ejection fraction in the range of 54%-74%, with values lower than that indicating patients in progressively worse conditions [@ejection2022]. Since all patients were either in the NYHA classification levels III or IV, we would expect that ejection fractions would likely be lower than expected for the average population; however, we also see that the distribution of ejection fraction for patients who died due to heart failure is lower than that of patients who did not die over the duration of the study.

```{r}
ggplot(data = heartfailure_data, aes(x = ejection_fraction, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Ejection Fraction Distribution of Patients", x = "Ejection Fraction", fill = "Death Event") 
```

### Platelets

A normal platelet count ranges from 150,000 to 450,000 platelets per micro liter of blood [@platelet1]. Research has shown that patients with thrombocytopaenia, or a low platelet count may be linked to higher rates of all-cause mortality [@platelet2]. From the plots, it appears that most patients fall within the normal platelet count level and no differences are immediately noticeable by eye.

```{r}
options(scipen = 999)

ggplot(data = heartfailure_data, aes(x = platelets, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Platelet Count Distribution of Patients", x = "Platelet Count (platelets/mcL)", fill = "Death Event") 
```


### Serum Creatinine

Serum Creatinine is a waste product of creatinine, a chemical made by the body that is used to supply energy to muscles. Normal results are 0.7 t0 1.3 mg/dL for men and 0.6 to 1.1 mg/dL for women [@serumc2021]. Serum creatinine above 1.5 mg/dL is associated with renal failure [@survive2017]. There is evidence that chronic kidney disease contributes to cardiac damage and that heart failure is also a major cause of chronic kidney disease [@serumc2004]. We see from the data that patients who died tended to have higher serum creatinine levels, which indicates that their kidneys would likely have been performing sub-optimally. 

```{r}
ggplot(data = heartfailure_data, aes(x = serum_creatinine, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Serum Creatinine Distribution of Patients", x = "Serum Creatinine (mg/dL)", fill = "Death Event") 
```


### Serum Sodium

The normal range for serum sodium is between 135 and 145 milliequivalents per liter (mEq/L) [@mayo2]. It appears that in general, patients who died over the duration of the study tended to have lower serum sodium levels than patients who did not die, so perhaps that could have indicated further progression of heart failure. Having low serum sodium levels, called hyponatremia is an indicator of heart failure, as it can cause fluids to accumulate in the body, which would dilute sodium in the body.

```{r}
ggplot(data = heartfailure_data, aes(x = serum_sodium, group = DEATH_EVENT, fill = DEATH_EVENT)) +
  geom_boxplot() + 
  scale_fill_manual(labels = c("Patient Did Not Die", "Patient Died"), values = c("lightblue", "pink")) +
  labs(title = "Serum Sodium Distribution of Patients", x = "Serum Sodium (mEql/L)", fill = "Death Event") 
```


## Model Setup

Upon basic analysis of our data, we know how various variables may be distributed as they relate to heart failure. We can now move on to the creation of our models. We must first randomly split our data into training and testing sets, create our recipes, and implement cross validation to assist with our models.



### Train Test Split
The first step towards the creation of our model is to first split our data into two groups: the training set and the test set. Splitting the data will allow our model to learn what characteristics contribute to a heart failure patient's death while preventing the potential over-fitting of data. After the model has been trained using the training set, we will be able to gauge the effectiveness of our model on the testing set. In order to keep our results consistent, we will also set a seed so that the data will be split at the same point every time. Finally, to ensure that both sets of data have the same number of deaths we will stratify our split on `DEATH_EVENT`.

```{r}
set.seed(12)  # Set seed to keep split consistent each time

# Creating initial 80/20 data split
heartfailure_split <- heartfailure_data %>%
  initial_split(prop = 0.80, strata = "DEATH_EVENT")

heartfailure_train <- training(heartfailure_split)  # Setting up training set
heartfailure_test <- testing(heartfailure_split)  # Setting up testing set
```

```{r}
dim(heartfailure_train)
dim(heartfailure_test)
```

After the split, there are 238 observations in the training set and 61 observations in the testing set.


### Building our Recipe

We can now put our predictors and response variables together to build the recipe which we will use throughout all our models. Since most models use pretty much the same predictors and conditions, we will create one recipe that can be implemented among the different models and which can be adjusted based on the models that we use. The recipe essentially contains the parts that we need in order to successfully create our machine learning models.

For our recipe, we will use the 11 predictors that we had previously mentioned: `age`, `amaemia`, `creatinine_phosphokinase`, `diabetes`, `ejection_fraction`, `high_blood_pressure`, `platelets`, `serum_creatinine`, `serum_sodium`, `sex`, and `smoking`.

For the recipe to work, we will also have to convert our categorical variables into dummy variables, and these variables are: `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking`.

We will also have to normalize our variables by centering and scaling the recipe. Finally, I have elected to upsample the data for `DEATH_EVENT` because of the imbalance in the number of patients who died in order to allow the model to train on both patients who died and survived.

```{r}
# Setting up recipe
heartfailure_recipe <- recipe(DEATH_EVENT ~ ., data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)

prep(heartfailure_recipe) %>%
  bake(new_data = heartfailure_train) %>%
  head() %>%
  kable() %>%
  kable_styling(full_width = F) %>%
  scroll_box(width = "100%", height = "200px")
```

### K-Fold Cross Validation
For many of our models, we may want to find the best hyperparameter that would allow us to have the most accurate model on our data, this process is known as hyperparameter tuning. Here k-fold cross-validation comes in handy. When using k-fold cross validation, it is common to use 10 folds, and that is what we will do here. We will stratify on the outcome `DEATH_EVENT` in order to perform the cross validation.

```{r}
heartfailure_folds <- vfold_cv(heartfailure_train, v = 10, strata = DEATH_EVENT)  # Create Folds
```


## Model Building

Now that we have all the initial setup completed, it is time for us to finally build our models. In this case, we have chosen to try seven different models to test their effectiveness at predicting heart failure deaths. The models all utilized the same basic recipe but applied to different techniques. To empirically gauge which model worked the best, we will use **accuracy** and **ROC-AUC** as our performance metrics. ROC-AUC is effective as a metric because it allows for the evaluation of a model in cases where the data is not perfectly balanced, much like in our given data set. ROC-AUC works by calculating the area under the curve for receiver the receiver operating characteristic curve, which shows the relationship between sensitivity and 1-specificity of our models. Sensitivity is also known as the true positive rate of our model while 1-specificity is the false positive rate of our model. When we test for accuracy, we are testing for how often our model predicted the correct outcome on our testing set.

The steps for building our model will be as follows: 

1. Set models up, indicating parameters that will be tuned later. We will also set the mode and engine of the model in the first step.

```{r}
# Logistic Regression
heart_log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")


# KNN Model
heart_knn_mod <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# LDA
heart_lda_mod <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

# QDA
heart_qda_mod <- discrim_quad() %>%
  set_engine("MASS") %>%
  set_mode("classification")

# Elastic Net
heart_en_mod <- multinom_reg(mixture = tune(), penalty = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# Random Forest
heart_rf_mod <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")


# Boosted Trees
heart_bt_mod <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")
```


2. Set up workflow and add model and recipe. 

```{r}
# Logistic Regression
heart_log_wkflow <- workflow() %>%
  add_model(heart_log_reg) %>%
  add_recipe(heartfailure_recipe)


# KNN Model
heart_knn_wkflow <- workflow() %>%
  add_model(heart_knn_mod) %>%
  add_recipe(heartfailure_recipe)

# LDA
heart_lda_wkflow <- workflow() %>%
  add_model(heart_lda_mod) %>%
  add_recipe(heartfailure_recipe)

# QDA
heart_qda_wkflow <- workflow() %>%
  add_model(heart_qda_mod) %>%
  add_recipe(heartfailure_recipe)

# Elastic Net
heart_en_wkflow <- workflow() %>%
  add_model(heart_en_mod) %>%
  add_recipe(heartfailure_recipe)

# Random Forest
heart_rf_wkflow <- workflow() %>%
  add_model(heart_rf_mod) %>% 
  add_recipe(heartfailure_recipe)

# Boosted Trees
heart_bt_wkflow <- workflow() %>%
  add_model(heart_bt_mod) %>%
  add_recipe(heartfailure_recipe)
```


3. Create tuning grid to set up range of data through which we will tune the model.

```{r}
# KNN Model
heart_neighbors_grid <- grid_regular(neighbors(range = c(1, 20)), levels = 20)

# Elastic Net
heart_en_grid <- grid_regular(penalty(range = c(0.02,3), trans = identity_trans()), mixture(range = c(0,1)), levels = 10)

# Random Forest
heart_rf_grid <- grid_regular(mtry(range = c(1, 10)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)

# Boosted Trees
heart_bt_grid <- grid_regular(mtry(range = c(1, 10)),
                              trees(range = c(100, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 5)
```


4. Tune model to find best fit for data and save models as rds.
```{r, eval=FALSE}
registerDoParallel(cores = 6)

# KNN Tuning
heart_tune_res_knn <- tune_grid(
  object = heart_knn_wkflow,
  resamples = heartfailure_folds,
  grid = heart_neighbors_grid,
  control = control_grid(verbose = TRUE)
)

# Elastic Net Tuning
heart_tune_res_en <- tune_grid(
  object = heart_en_wkflow,
  resamples = heartfailure_folds,
  grid = heart_en_grid,
  control = control_grid(verbose = TRUE)
)

# Random Forest Tuning
heart_tune_res_rf <- tune_grid(
  object = heart_rf_wkflow,
  resamples = heartfailure_folds,
  grid = heart_rf_grid,
  control = control_grid(verbose = TRUE)
)

# Boosted Trees Tuning
heart_tune_res_bt <- tune_grid(
  object = heart_bt_wkflow,
  resamples = heartfailure_folds,
  grid = heart_bt_grid,
  control = control_grid(verbose = TRUE)
)

# Writing results to rds files
# KNN Model
write_rds(heart_tune_res_knn, file = "heart_knn.rds")

# Elastic Net
write_rds(heart_tune_res_en, file = "heart_en.rds")

# Random Forest
write_rds(heart_tune_res_rf, file = "heart_rf.rds")

# Boosted Trees
write_rds(heart_tune_res_bt, file = "heart_bt.rds")
```


5. Load tuned model back in

```{r}
# Load in tuned KNN
heart_knn_tuned <- read_rds(file = "heart_knn.rds")

# Load in tuned Elastic Net
heart_en_tuned <- read_rds(file = "heart_en.rds")

# Load in tuned Random Forest
heart_rf_tuned <- read_rds(file = "heart_rf.rds")

# Load in tuned Boosted Trees
heart_bt_tuned <- read_rds(file = "heart_bt.rds")

```


6. Perform analysis on data to determine which model fit best for data.

```{r}
# Collect Metrics
# KNN
best_knn <- show_best(heart_knn_tuned, metric = "roc_auc")

# Elastic Net 
best_en <- show_best(heart_en_tuned, metric = "roc_auc")

# Random Forest
best_rf <- show_best(heart_rf_tuned, metric = "roc_auc")

# Gradient-Boosted Trees
best_bt <- show_best(heart_bt_tuned, metric = "roc_auc")
```


## Model Evaluation

### K-Nearest Neighbor Plots
```{r}
# KNN Autoplot
autoplot(heart_knn_tuned)
```

For the KNN Plot, we see that increasing the number of neighbors increases the ROC AUC but we also see that the accuracy of the model does not become better after 5 neighbors. However, when compared to our other models, we also see that the best ROC AUC is lower so it may not be the best model for the purposes of predicting heart failure deaths.


### Elastic Net Plots

```{r}
# Elastic Net Autoplot
autoplot(heart_en_tuned, metric = "roc_auc")
```

From our Elastic Net Plot, we are able to tune the proportion of lasso penalty and the amount of regularization. From the plot, we are able to see that models with lower lasso penalty proportions tend to have higher ROC AUC values. We are also able to see that as the amount of regularization increases, we also tend to see models with lower ROC AUC values. From the different models we tuned on we see that the ideal combiantion for the best mean ROC AUC value is a penalty if 0.341 amd a mixture of 0, meaning 0.351 for the lasso penalty and no regularization.

### Random Forest Plots

```{r}
# Random Forest Autoplot
autoplot(heart_rf_tuned, metric = "roc_auc")
```

For Random Forest models, we are able to tune three hyperparameters: the minimum size of a node through `min_n`, the number of trees through `trees`, and the number of randomly sampled predictors through `mtry`. From the plot, we see that there appears to be a large range of ROC AUC values that are obtained based on the different numbers. It appears that increasing the minimum node size tends to increase the ROC AUC values of the models. Having low numbers of randomly selected predictors also appears to increase model performance. From our different models, the random forest with the best performance was the model that randomly sampled 1 predictor, had a model with 155 trees and had a minimum node size of 20.

### Gradient-Boosted Trees Plots

```{r}
# Gradient-Boosted Trees Autoplot
autoplot(heart_bt_tuned, metric = "roc_auc")
```

For Gradient-Boosted trees, we are able to tune the learning rate (`learn_rate`), number of trees (`trees`), and the number of randomly selected predictors (`min_n`). We can see that with an extremely low learning rate, the model did quite poorly. However, it appears that with higher learnign rates, the performance of the model significantly improves and for the most part, models perform quite well despite the number of trees used in the model. We also see that as the number of randomly selected predictors increase, we tend to get decreasing performance of the model for gradient-boosted trees. The best we fit appears to be the model with 1 randomly selected predictor, 775 trees, and a learn rate of 0.000562.


```{r}
# Selecting best models and applying them to workflow
# KNN
best_heart_neighbors <- select_best(heart_tune_res_knn,
                                    desc(neighbors),
                                    metric = "roc_auc")

final_heart_knn_model <- finalize_workflow(heart_knn_wkflow, best_heart_neighbors)


# Elastic Net
best_heart_en <- select_best(heart_tune_res_en,
                             metric = "roc_auc",
                             penalty,
                             mixture)

final_heart_en_model <- finalize_workflow(heart_en_wkflow, best_heart_en)


# Random Forest
best_heart_rf <- select_best(heart_tune_res_rf, metric = "roc_auc")

final_heart_rf_model <- finalize_workflow(heart_rf_wkflow, best_heart_rf)


# Gradient-Boosted Trees
best_heart_bt <- select_best(heart_tune_res_bt, metric = "roc_auc")

final_heart_bt_model <- finalize_workflow(heart_bt_wkflow, best_heart_bt)
```


```{r}
# Fit models together
# Logistic Regression
heart_log_fit <- fit(heart_log_wkflow, heartfailure_train)

# KNN Model
heart_knn_fit <- fit(final_heart_knn_model, heartfailure_train)

# LDA
heart_lda_fit <- fit(heart_lda_wkflow, heartfailure_train)

# QDA
heart_qda_fit <- fit(heart_qda_wkflow, heartfailure_train)

# Elastic Net
heart_en_fit <- fit(final_heart_en_model, heartfailure_train)

# Random Forest
heart_rf_fit <- fit(final_heart_rf_model, heartfailure_train)

# Boosted Trees
heart_bt_fit <- fit(final_heart_bt_model, heartfailure_train)

```

### ROC AUC of Various Models

In the table below, we summarize the ROC AUC of each model on the training data. We see that the top two models in our case in terms of ROC AUC were random forest models and Gradient-Boosted Trees so we will select those two methods as models that we will fit on our testing data and further analyze.

```{r}
# Evaluating ROC AUC of different models
# Logistic Regression
heart_log_train_results <- augment(heart_log_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

# KNN Model
heart_knn_train_results <- augment(heart_knn_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)
  
# LDA
heart_lda_train_results <- augment(heart_lda_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

# QDA
heart_qda_train_results <- augment(heart_qda_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

# Elastic Net
heart_en_train_results <- augment(heart_en_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

# Random Forest
heart_rf_train_results <- augment(heart_rf_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

# Boosted Trees
heart_bt_train_resutls <- augment(heart_bt_fit, new_data = heartfailure_train) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)
```

```{r}
heart_models <- c("Logistic Regression", "K-Nearest Neighbor", "Linear Discriminant Analysis", "Quadratic Discriminant Analysis", "Elastic Net", "Random Forest", "Gradient-Boosted Trees")

heart_model_roc_auc <- c(heart_log_train_results$.estimate, heart_knn_train_results$.estimate, heart_lda_train_results$.estimate, heart_qda_train_results$.estimate, heart_en_train_results$.estimate, heart_rf_train_results$.estimate, heart_bt_train_resutls$.estimate)

heart_train_results <- tibble(Model = heart_models, ROC_AUC = heart_model_roc_auc) %>% 
  arrange(-heart_model_roc_auc) %>%
  gt() %>%
  tab_header(title = "ROC AUC Values of Different Models") %>%
  gt_theme_nytimes()

heart_train_results 
```

## Fitting Testing Data to Models

Now that we have chosen random forests and boosted-gradient trees as our two best models, we will fit testing data to them and analyze their results. After fittign the testing data, we are then able to observe the ROC AUC of the model on testing data. We see that as with the training set, our random forest model performed slightly better than our gradient-boosted trees model with an ROC AUC value of 0.746 compared to 0.734 respectively. Conversely, we see that the accuracy of gradient-boosted trees (73.4%) is slightly higher than for random forests (70.5%).

```{r}
tibble(Model = c("Random Forest", "Gradient-Boosted Trees"), ROC_AUC = c(heart_rf_roc_auc$.estimate, heart_bt_roc_auc$.estimate), Accuracy = c(heart_rf_accuracy$.estimate, heart_bt_accuracy$.estimate)) %>%
  gt() %>%
  tab_header(title = "Performance Metrics of Tested Models") %>%
  gt_theme_nytimes()
```


### Random Forest
```{r}
# ROC AUC Plot
# Random Forest
heart_rf_roc_auc <- augment(heart_rf_fit, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

heart_rf_accuracy <- augment(heart_rf_fit, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

final_rf_model_test <- augment(heart_rf_fit, heartfailure_test)
```

From the confusion matrix below, we are able to see how our random forest actually did on our model in terms of prediction. We see that the model has a false positive rate about 13.1% of the time and a false negative rate about 16.4% of the time. As shown earlier, this model has an accuracy of 70.5%. It is important to note, however, that our testing set is quite small at only 61 values, meaning that accuracy is very sensitive depending on the results of the model.
```{r}
conf_mat(final_rf_model_test, truth = DEATH_EVENT, .pred_class) %>% autoplot(type = "heatmap")
```

In the figure below, we plot the ROC curve. The more the ROC curve goes up and to the left, the better the model. Although this model does not follow that trajectory fully, its shape and the ROC AUC value of 0.746 still indicate a model that is relatively efficient at predicting the heart failure outcome of patients.

```{r}
augment(heart_rf_fit, new_data = heartfailure_test) %>%
  roc_curve(DEATH_EVENT, estimate = .pred_1) %>%
  autoplot() 
```

### Gradient-Boosted Trees
```{r}
heart_bt_accuracy <- augment(heart_bt_fit, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc <- augment(heart_bt_fit, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)
```

The confusion matrix below breaks down the predictions made by the Gradient-Boosted Trees model. Again, it is important to note that accuracy may be signifcantly affected purely because of the low number of observations in teh training set, but this model exhibited 73.4% accuracy on the testing data.


```{r}
final_bt_model_test <- augment(heart_bt_fit, heartfailure_test)

conf_mat(final_bt_model_test, truth = DEATH_EVENT, .pred_class) %>% autoplot(type = "heatmap")
```

Below, we see the ROC curve plotted for the gradient-boosted trees model. The model had an ROC AUC of 0.734, and while not perfect, demonstrates the general shape that we want to see from the ROC curve.

```{r}
augment(heart_bt_fit, new_data = heartfailure_test) %>%
  roc_curve(DEATH_EVENT, estimate = .pred_1) %>%
  autoplot()
```


### Variable Importance Plot

When creating a random forest model, we are able to create a variable importance plot, which allows us to visualize which variables were most effective in improving the model. We are able to see that variables which were dummy coded (high blood pressure, diabetes, smoking, and sex) appear to have relatively small effects in predicting the outcome of heart failure patients. We are also able to see the ranking of the importance of the variables in the plot with the variable most useful in improving the model at the top. 

```{r}
heart_rf_fit %>%
  extract_fit_engine() %>%
  vip(aesthetics = list(fill = "pink", color = "lightblue"))
```

Knowing that some variables are more effective at predicting patient outcomes, we will elect to try out different random forest models with different numbers of predictors. We will test out all models that range from the 2 most useful predictors to the 6 most useful predictors to see if we can improve the efficacy of our models with simpler recipes. The reason we want to look into simplifying our models is that overly-complex models may be more likely to over fit as they begin to fit the noise of the training data instead of the underlying patterns which we look for [@simpler]. Having simpler models will also improve interpretability and in the medical setting may allow doctors to more easily diagnose patients with less tests necessary, which would both save time and cut costs. Since it was clear that random forest and boosted-gradient trees were superior previously, we will continue focusing on these two algorithms as we test our data on simpler models.


## Testing Simpler Models

We will proceed with testing simpler models to evaluate if having less predictors in our recipe allows us to create models that allow us to better predict the outcome of patients. For each version of the model, we will output the best model for both random forest and gradient-boosted models and we will then further analyze the best models we get from each algorithm.

### 2 Most Important Variables:
```{r, eval = FALSE}
heartfailure_recipe2 <- recipe(DEATH_EVENT ~ serum_creatinine + ejection_fraction, data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)


# Random Forest
heart_rf_mod2 <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")



# Boosted Trees
heart_bt_mod2 <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")



# Random Forest
heart_rf_wkflow2 <- workflow() %>%
  add_model(heart_rf_mod2) %>% 
  add_recipe(heartfailure_recipe2)

# Boosted Trees
heart_bt_wkflow2 <- workflow() %>%
  add_model(heart_bt_mod2) %>%
  add_recipe(heartfailure_recipe2)


# Random Forest
heart_rf_grid2 <- grid_regular(mtry(range = c(1,1)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)


# Boosted Trees
heart_bt_grid2 <- grid_regular(mtry(range = c(1,1)),
                              trees(range = c(500, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 10)

# Random Forest Tuning
heart_tune_res_rf2 <- tune_grid(
  object = heart_rf_wkflow2,
  resamples = heartfailure_folds,
  grid = heart_rf_grid2,
  control = control_grid(verbose = TRUE)
)


# Boosted Trees Tuning
heart_tune_res_bt2 <- tune_grid(
  object = heart_bt_wkflow2,
  resamples = heartfailure_folds,
  grid = heart_bt_grid2,
  control = control_grid(verbose = TRUE)
)


# Writing results to rds files
# Random Forest
write_rds(heart_tune_res_rf2, file = "heart_rf.rds2")


# Boosted Trees
write_rds(heart_tune_res_bt2, file = "heart_bt.rds2")

```

```{r}
# Load in tuned Random Forest
heart_rf_tuned2 <- read_rds(file = "heart_rf.rds2")


# Load in tuned Boosted Trees
heart_bt_tuned2 <- read_rds(file = "heart_bt.rds2")


# Random Forest
show_best(heart_rf_tuned2, metric = "roc_auc", n=1)


# Gradient-Boosted Trees
show_best(heart_bt_tuned2, metric = "roc_auc", n =1)


# Random Forest
best_heart_rf2 <- select_best(heart_tune_res_rf2, metric = "roc_auc")

final_heart_rf_model2 <- finalize_workflow(heart_rf_wkflow2, best_heart_rf2)



# Gradient-Boosted Trees
best_heart_bt2 <- select_best(heart_tune_res_bt2, metric = "roc_auc")

final_heart_bt_model2 <- finalize_workflow(heart_bt_wkflow2, best_heart_bt2)


heart_rf_roc_auc2 <- augment(heart_rf_fit2, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)


heart_rf_accuracy2 <- augment(heart_rf_fit2, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_accuracy2 <- augment(heart_bt_fit2, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc2 <- augment(heart_bt_fit2, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

final_rf_model_test2 <- augment(heart_rf_fit2, heartfailure_test)

heart_rf_roc_auc2 <- augment(heart_rf_fit2, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)
```



### 3 Most Important Variables:

```{r, eval=FALSE}

heartfailure_recipe3 <- recipe(DEATH_EVENT ~ serum_creatinine + ejection_fraction + serum_sodium, data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)

# Random Forest


heart_rf_mod3 <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")


# Boosted Trees


heart_bt_mod3 <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Random Forest


heart_rf_wkflow3 <- workflow() %>%
  add_model(heart_rf_mod3) %>% 
  add_recipe(heartfailure_recipe3)

# Boosted Trees


heart_bt_wkflow3 <- workflow() %>%
  add_model(heart_bt_mod3) %>%
  add_recipe(heartfailure_recipe3)

# Random Forest


heart_rf_grid3 <- grid_regular(mtry(range = c(1, 2)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)

# Boosted Trees

heart_bt_grid3 <- grid_regular(mtry(range = c(1, 2)),
                              trees(range = c(500, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 10)

# Random Forest Tuning


heart_tune_res_rf3 <- tune_grid(
  object = heart_rf_wkflow3,
  resamples = heartfailure_folds,
  grid = heart_rf_grid3,
  control = control_grid(verbose = TRUE)
)

# Boosted Trees Tuning

heart_tune_res_bt3 <- tune_grid(
  object = heart_bt_wkflow3,
  resamples = heartfailure_folds,
  grid = heart_bt_grid3,
  control = control_grid(verbose = TRUE)
)

# Writing results to rds files
# Random Forest

write_rds(heart_tune_res_rf3, file = "heart_rf.rds3")

# Boosted Trees

write_rds(heart_tune_res_bt3, file = "heart_bt.rds3")
```

```{r}
# Load in tuned Random Forest

heart_rf_tuned3 <- read_rds(file = "heart_rf.rds3")

# Load in tuned Boosted Trees

heart_bt_tuned3 <- read_rds(file = "heart_bt.rds3")

# Random Forest

show_best(heart_rf_tuned3, metric = "roc_auc", n = 1)

# Gradient-Boosted Trees

show_best(heart_bt_tuned3, metric = "roc_auc", n = 1)

# Random Forest


best_heart_rf3 <- select_best(heart_tune_res_rf3, metric = "roc_auc")

final_heart_rf_model3 <- finalize_workflow(heart_rf_wkflow3, best_heart_rf3)


# Gradient-Boosted Trees
best_heart_bt3 <- select_best(heart_tune_res_bt3, metric = "roc_auc")

final_heart_bt_model3 <- finalize_workflow(heart_bt_wkflow3, best_heart_bt3)


heart_rf_roc_auc3 <- augment(heart_rf_fit3, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)


heart_rf_accuracy3 <- augment(heart_rf_fit3, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_accuracy3 <- augment(heart_bt_fit3, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc3 <- augment(heart_bt_fit3, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

final_rf_model_test3 <- augment(heart_rf_fit3, heartfailure_test)
```

### 4 Most Important Variables:

```{r, eval=FALSE}

heartfailure_recipe4 <- recipe(DEATH_EVENT ~ serum_creatinine + ejection_fraction + serum_sodium + age, data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)

# Random Forest


heart_rf_mod4 <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")


# Boosted Trees


heart_bt_mod4 <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Random Forest


heart_rf_wkflow4 <- workflow() %>%
  add_model(heart_rf_mod4) %>% 
  add_recipe(heartfailure_recipe4)

# Boosted Trees


heart_bt_wkflow4 <- workflow() %>%
  add_model(heart_bt_mod4) %>%
  add_recipe(heartfailure_recipe4)

# Random Forest


heart_rf_grid4 <- grid_regular(mtry(range = c(1, 3)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)

# Boosted Trees

heart_bt_grid4 <- grid_regular(mtry(range = c(1, 3)),
                              trees(range = c(500, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 10)

# Random Forest Tuning


heart_tune_res_rf4 <- tune_grid(
  object = heart_rf_wkflow4,
  resamples = heartfailure_folds,
  grid = heart_rf_grid4,
  control = control_grid(verbose = TRUE)
)

# Boosted Trees Tuning

heart_tune_res_bt4 <- tune_grid(
  object = heart_bt_wkflow4,
  resamples = heartfailure_folds,
  grid = heart_bt_grid4,
  control = control_grid(verbose = TRUE)
)

# Writing results to rds files
# Random Forest

write_rds(heart_tune_res_rf4, file = "heart_rf.rds4")

# Boosted Trees

write_rds(heart_tune_res_bt4, file = "heart_bt.rds4")
```

```{r}
# Load in tuned Random Forest

heart_rf_tuned4 <- read_rds(file = "heart_rf.rds4")

# Load in tuned Boosted Trees

heart_bt_tuned4 <- read_rds(file = "heart_bt.rds4")

# Random Forest

show_best(heart_rf_tuned4, metric = "roc_auc", n = 1)

# Gradient-Boosted Trees

show_best(heart_bt_tuned4, metric = "roc_auc", n = 1)

# Random Forest


best_heart_rf4 <- select_best(heart_tune_res_rf4, metric = "roc_auc")

final_heart_rf_model4 <- finalize_workflow(heart_rf_wkflow4, best_heart_rf4)


# Gradient-Boosted Trees
best_heart_bt4 <- select_best(heart_tune_res_bt4, metric = "roc_auc")

final_heart_bt_model4 <- finalize_workflow(heart_bt_wkflow4, best_heart_bt4)

heart_rf_roc_auc4 <- augment(heart_rf_fit4, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)


heart_rf_accuracy4 <- augment(heart_rf_fit4, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_accuracy4 <- augment(heart_bt_fit4, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc4 <- augment(heart_bt_fit4, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

final_rf_model_test4 <- augment(heart_rf_fit4, heartfailure_test)
```

### 5 Most Important Variables
```{r, eval = FALSE}

heartfailure_recipe5 <- recipe(DEATH_EVENT ~ serum_creatinine + ejection_fraction + serum_sodium + age + creatinine_phosphokinase, data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)

# Random Forest
heart_rf_mod5 <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")


# Boosted Trees


heart_bt_mod5 <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Random Forest


heart_rf_wkflow5 <- workflow() %>%
  add_model(heart_rf_mod5) %>% 
  add_recipe(heartfailure_recipe5)

# Boosted Trees


heart_bt_wkflow5 <- workflow() %>%
  add_model(heart_bt_mod5) %>%
  add_recipe(heartfailure_recipe5)

# Random Forest


heart_rf_grid5 <- grid_regular(mtry(range = c(1, 4)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)

# Boosted Trees

heart_bt_grid5 <- grid_regular(mtry(range = c(1, 4)),
                              trees(range = c(500, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 10)

# Random Forest Tuning


heart_tune_res_rf5 <- tune_grid(
  object = heart_rf_wkflow5,
  resamples = heartfailure_folds,
  grid = heart_rf_grid5,
  control = control_grid(verbose = TRUE)
)

# Boosted Trees Tuning

heart_tune_res_bt5 <- tune_grid(
  object = heart_bt_wkflow5,
  resamples = heartfailure_folds,
  grid = heart_bt_grid5,
  control = control_grid(verbose = TRUE)
)

# Writing results to rds files
# Random Forest

write_rds(heart_tune_res_rf5, file = "heart_rf.rds5")

# Boosted Trees

write_rds(heart_tune_res_bt5, file = "heart_bt.rds5")
```

```{r}
# Load in tuned Random Forest

heart_rf_tuned5 <- read_rds(file = "heart_rf.rds5")

# Load in tuned Boosted Trees

heart_bt_tuned5 <- read_rds(file = "heart_bt.rds5")

# Random Forest

show_best(heart_rf_tuned5, metric = "roc_auc", n = 1)

# Gradient-Boosted Trees

show_best(heart_bt_tuned5, metric = "roc_auc", n = 1)

# Random Forest


best_heart_rf5 <- select_best(heart_tune_res_rf5, metric = "roc_auc")

final_heart_rf_model5 <- finalize_workflow(heart_rf_wkflow5, best_heart_rf5)


# Gradient-Boosted Trees
best_heart_bt5 <- select_best(heart_tune_res_bt5, metric = "roc_auc")

final_heart_bt_model5 <- finalize_workflow(heart_bt_wkflow5, best_heart_bt5)


heart_rf_roc_auc5 <- augment(heart_rf_fit5, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)


heart_rf_accuracy5 <- augment(heart_rf_fit5, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_accuracy5 <- augment(heart_bt_fit5, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc5 <- augment(heart_bt_fit5, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

final_rf_model_test5 <- augment(heart_rf_fit5, heartfailure_test)
```

### 6 Most Important Variables

```{r, eval = FALSE}

heartfailure_recipe6 <- recipe(DEATH_EVENT ~ serum_creatinine + ejection_fraction + serum_sodium + age + creatinine_phosphokinase + platelets, data = heartfailure_train) %>%
  step_dummy(all_nominal_predictors()) %>%   #creating dummy variables
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>%
  step_upsample(DEATH_EVENT, over_ratio = 1)

# Random Forest


heart_rf_mod6 <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")


# Boosted Trees


heart_bt_mod6 <- boost_tree(mtry = tune(), trees = tune(), learn_rate = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# Random Forest


heart_rf_wkflow6 <- workflow() %>%
  add_model(heart_rf_mod6) %>% 
  add_recipe(heartfailure_recipe6)

# Boosted Trees


heart_bt_wkflow6 <- workflow() %>%
  add_model(heart_bt_mod6) %>%
  add_recipe(heartfailure_recipe6)

# Random Forest


heart_rf_grid6 <- grid_regular(mtry(range = c(1, 5)),
                              trees(range = c(50, 1000)),
                              min_n(range = c(5, 25)), levels = 10)

# Boosted Trees

heart_bt_grid6 <- grid_regular(mtry(range = c(1, 5)),
                              trees(range = c(500, 1000)),
                              learn_rate(range = c(-10, -1)),
                              levels = 10)

# Random Forest Tuning


heart_tune_res_rf6 <- tune_grid(
  object = heart_rf_wkflow6,
  resamples = heartfailure_folds,
  grid = heart_rf_grid6,
  control = control_grid(verbose = TRUE)
)

# Boosted Trees Tuning

heart_tune_res_bt6 <- tune_grid(
  object = heart_bt_wkflow6,
  resamples = heartfailure_folds,
  grid = heart_bt_grid6,
  control = control_grid(verbose = TRUE)
)

# Writing results to rds files
# Random Forest

write_rds(heart_tune_res_rf6, file = "heart_rf.rds6")

# Boosted Trees

write_rds(heart_tune_res_bt6, file = "heart_bt.rds6")
```

```{r}
# Load in tuned Random Forest

heart_rf_tuned6 <- read_rds(file = "heart_rf.rds6")

# Load in tuned Boosted Trees

heart_bt_tuned6 <- read_rds(file = "heart_bt.rds6")

# Random Forest

show_best(heart_rf_tuned6, metric = "roc_auc", n = 1)

# Gradient-Boosted Trees

show_best(heart_bt_tuned6, metric = "roc_auc", n = 1)

# Random Forest


best_heart_rf6 <- select_best(heart_tune_res_rf6, metric = "roc_auc")

final_heart_rf_model6 <- finalize_workflow(heart_rf_wkflow6, best_heart_rf4)


# Gradient-Boosted Trees
best_heart_bt6 <- select_best(heart_tune_res_bt6, metric = "roc_auc")

final_heart_bt_model6 <- finalize_workflow(heart_bt_wkflow6, best_heart_bt6)


heart_rf_roc_auc6 <- augment(heart_rf_fit6, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)


heart_rf_accuracy6 <- augment(heart_rf_fit6, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_accuracy6 <- augment(heart_bt_fit6, new_data = heartfailure_test) %>%
  accuracy(DEATH_EVENT, estimate = .pred_class)

heart_bt_roc_auc6 <- augment(heart_bt_fit6, new_data = heartfailure_test) %>%
  roc_auc(DEATH_EVENT, estimate = .pred_1)

final_rf_model_test6 <- augment(heart_rf_fit6, heartfailure_test)
```

### Analysis of Simpler Models

```{r, eval=FALSE}
variables_used <- c("Original Model (11 Predictors)", "2", "3", "4", "5", "6")
rf_roc_auc <- c(heart_rf_roc_auc$.estimate, heart_rf_roc_auc2$.estimate, heart_rf_roc_auc3$.estimate, heart_rf_roc_auc4$.estimate, heart_rf_roc_auc5$.estimate, heart_rf_roc_auc6$.estimate)
rf_accuracy <- c(heart_rf_accuracy$.estimate, heart_rf_accuracy2$.estimate, heart_rf_accuracy3$.estimate, heart_rf_accuracy4$.estimate, heart_rf_accuracy5$.estimate, heart_rf_accuracy6$.estimate)
rf_better <- c("", "Yes", "Yes", "No", "Yes", "No")
bt_roc_auc <- c(heart_bt_roc_auc$.estimate, heart_bt_roc_auc2$.estimate, heart_bt_roc_auc3$.estimate, heart_bt_roc_auc4$.estimate, heart_bt_roc_auc5$.estimate, heart_bt_roc_auc6$.estimate)
bt_better <- c("", "Yes", "Yes" ,"Yes", "Yes", "No")


tibble(Predictor_Number = variables_used,
       ROC_AUC = rf_roc_auc,
       Improvement = rf_better
       ) %>%
  gt() %>%
  gt_theme_nytimes() %>%
  tab_header(title = "Performance of RF Models with Different Numbers of Best Variables") %>%
  cols_label(ROC_AUC = "ROC AUC",
    Improvement = "Improvement on Full Model",
    Predictor_Number = "Number of Best Predictors") 

tibble(Predictor_Number = variables_used,
       ROC_AUC_bt = bt_roc_auc,
       Improvement_bt = bt_better) %>%
  gt() %>%
  gt_theme_nytimes() %>%
  tab_header(title = "Performance of BT Models with Different Numbers of Best Variables") %>%
  cols_label(ROC_AUC_bt = "ROC AUC",
    Improvement_bt = "Improvement on Full Model",
    Predictor_Number = "Number of Best Predictors") 
round(rf_roc_auc, 4)
```

It appears that for both random forest and boosted-gradient tree algorithms the best of the simplified models were the ones that used the two best predictors, which were Serum Creatinine and Ejection Fraction. Additionally, we see that these algorithms actually had substantially better ROC AUC values compared to the full models with values of 0.855 compared to 0.746 for random forest models and 0.826 compared to 0.734 for gradient-boosted tree models.

## Conclusion

## References
